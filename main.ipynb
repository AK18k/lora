{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK18k/lora/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19mBVBKOU8ul",
        "outputId": "5f421277-f560-44cd-f9b8-c5ab4339d970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lora'...\n",
            "remote: Enumerating objects: 1405, done.\u001b[K\n",
            "remote: Counting objects: 100% (1405/1405), done.\u001b[K\n",
            "remote: Compressing objects: 100% (999/999), done.\u001b[K\n",
            "remote: Total 1405 (delta 389), reused 1377 (delta 371), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1405/1405), 23.87 MiB | 16.50 MiB/s, done.\n",
            "Resolving deltas: 100% (389/389), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AK18k/lora"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install progress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGTMRSDrVdwA",
        "outputId": "87cb4b81-0c75-49ed-f1a3-5923c0dad8fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting progress\n",
            "  Downloading progress-1.6.tar.gz (7.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: progress\n",
            "  Building wheel for progress (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progress: filename=progress-1.6-py3-none-any.whl size=9611 sha256=10a4294f0d7e5d71323e9331172ed093d121b977e4bb8b7fb266b2f6531f68b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/68/5f/c339b20a41659d856c93ccdce6a33095493eb82c3964aac5a1\n",
            "Successfully built progress\n",
            "Installing collected packages: progress\n",
            "Successfully installed progress-1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd ./lora/examples/NLG\n",
        "# !ls '*.sh'\n",
        "!bash ./lora/examples/NLG/download_pretrained_checkpoints.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aI_0ZMPVrux",
        "outputId": "5eeff236-b231-418c-97d6-ca7956f0cd57"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading pretrained model checkpoints...\n",
            "--2023-06-03 15:08:14--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.128.136, 52.216.211.248, 52.216.109.133, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.128.136|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 548118077 (523M) [application/octet-stream]\n",
            "Saving to: ‘gpt2-pytorch_model.bin’\n",
            "\n",
            "gpt2-pytorch_model. 100%[===================>] 522.73M  33.2MB/s    in 17s     \n",
            "\n",
            "2023-06-03 15:08:31 (31.4 MB/s) - ‘gpt2-pytorch_model.bin’ saved [548118077/548118077]\n",
            "\n",
            "--2023-06-03 15:08:31--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.138.101, 52.217.233.176, 52.217.75.150, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.138.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520013706 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘gpt2-medium-pytorch_model.bin’\n",
            "\n",
            "gpt2-medium-pytorch 100%[===================>]   1.42G  33.6MB/s    in 44s     \n",
            "\n",
            "2023-06-03 15:09:16 (33.2 MB/s) - ‘gpt2-medium-pytorch_model.bin’ saved [1520013706/1520013706]\n",
            "\n",
            "--2023-06-03 15:09:16--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-pytorch_model.bin\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.202.80, 54.231.162.176, 52.217.139.208, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.202.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3247202234 (3.0G) [application/octet-stream]\n",
            "Saving to: ‘gpt2-large-pytorch_model.bin’\n",
            "\n",
            "gpt2-large-pytorch_ 100%[===================>]   3.02G  33.9MB/s    in 94s     \n",
            "\n",
            "2023-06-03 15:10:50 (33.1 MB/s) - ‘gpt2-large-pytorch_model.bin’ saved [3247202234/3247202234]\n",
            "\n",
            "script complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./lora/examples/NLG/create_datasets.sh"
      ],
      "metadata": {
        "id": "EuLfqW_PYwlz",
        "outputId": "3af487de-c42f-4575-8d03-526c35e275cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating e2e datasets...\n",
            "train...\n",
            "test...\n",
            "valid...\n",
            "creating webnlg datasets...\n",
            "train...\n",
            "cate Airport 1090\n",
            "cate Astronaut 530\n",
            "cate Building 972\n",
            "cate City 243\n",
            "cate ComicsCharacter 285\n",
            "cate Food 1424\n",
            "cate Monument 267\n",
            "cate SportsTeam 786\n",
            "cate University 406\n",
            "cate WrittenWork 937\n",
            "test...\n",
            "cate Airport 136\n",
            "cate Monument 33\n",
            "cate Building 120\n",
            "cate WrittenWork 116\n",
            "cate SportsTeam 98\n",
            "cate University 51\n",
            "cate Astronaut 66\n",
            "cate ComicsCharacter 35\n",
            "cate City 139\n",
            "cate Food 177\n",
            "cate Politician 209\n",
            "cate MeanOfTransportation 198\n",
            "cate Athlete 159\n",
            "cate Artist 213\n",
            "cate CelestialBody 112\n",
            "valid...\n",
            "cate Airport 136\n",
            "cate Astronaut 67\n",
            "cate Building 123\n",
            "cate City 31\n",
            "cate ComicsCharacter 37\n",
            "cate Food 178\n",
            "cate Monument 32\n",
            "cate SportsTeam 99\n",
            "cate University 51\n",
            "cate WrittenWork 118\n",
            "creating dart datasets...\n",
            "train...\n",
            "unique source is 30526\n",
            "test...\n",
            "unique source is 6959\n",
            "valid...\n",
            "unique source is 2768\n",
            "script complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash /content/lora/examples/NLG/eval/download_evalscript.sh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4rEoOZvysmM",
        "outputId": "5cc5cc17-ee9d-4d89-ea71-aa71d5410b1b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing evaluation dependencies\n",
            "downloading e2e-metrics...\n",
            "Cloning into 'e2e'...\n",
            "remote: Enumerating objects: 909, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 909 (delta 0), reused 1 (delta 0), pack-reused 906\u001b[K\n",
            "Receiving objects: 100% (909/909), 106.78 MiB | 14.45 MiB/s, done.\n",
            "Resolving deltas: 100% (492/492), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r e2e/requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r e2e/requirements.txt (line 2)) (0.19.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from -r e2e/requirements.txt (line 3)) (0.18.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r e2e/requirements.txt (line 1)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r e2e/requirements.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r e2e/requirements.txt (line 1)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r e2e/requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r e2e/requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r e2e/requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r e2e/requirements.txt (line 1)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r e2e/requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r e2e/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r e2e/requirements.txt (line 2)) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r e2e/requirements.txt (line 2)) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r e2e/requirements.txt (line 2)) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r e2e/requirements.txt (line 2)) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r e2e/requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r e2e/requirements.txt (line 1)) (1.16.0)\n",
            "downloading GenerationEval for webnlg and dart...\n",
            "Cloning into 'GenerationEval'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 82 (delta 36), reused 51 (delta 15), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (82/82), 221.17 KiB | 5.27 MiB/s, done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nltk==3.5 (from -r requirements.txt (line 1))\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyter3==0.3 (from -r requirements.txt (line 2))\n",
            "  Downloading pyter3-0.3-py3-none-any.whl (4.1 kB)\n",
            "Collecting razdel==0.5.0 (from -r requirements.txt (line 3))\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting tabulate==0.8.7 (from -r requirements.txt (line 4))\n",
            "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
            "Collecting bert-score==0.3.5 (from -r requirements.txt (line 5))\n",
            "  Downloading bert_score-0.3.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.5->-r requirements.txt (line 1)) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.5->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nltk==3.5->-r requirements.txt (line 1)) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.5->-r requirements.txt (line 1)) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.5->-r requirements.txt (line 5)) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.5->-r requirements.txt (line 5)) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.5->-r requirements.txt (line 5)) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.5->-r requirements.txt (line 5)) (2.27.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.5->-r requirements.txt (line 5)) (3.7.1)\n",
            "Collecting transformers>=3.0.0 (from bert-score==0.3.5->-r requirements.txt (line 5))\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score==0.3.5->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score==0.3.5->-r requirements.txt (line 5)) (2022.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score==0.3.5->-r requirements.txt (line 5)) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score==0.3.5->-r requirements.txt (line 5)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score==0.3.5->-r requirements.txt (line 5)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score==0.3.5->-r requirements.txt (line 5)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score==0.3.5->-r requirements.txt (line 5)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score==0.3.5->-r requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert-score==0.3.5->-r requirements.txt (line 5)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert-score==0.3.5->-r requirements.txt (line 5)) (16.0.5)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=3.0.0->bert-score==0.3.5->-r requirements.txt (line 5))\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score==0.3.5->-r requirements.txt (line 5)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score==0.3.5->-r requirements.txt (line 5)) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=3.0.0->bert-score==0.3.5->-r requirements.txt (line 5))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.5->-r requirements.txt (line 5)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.5->-r requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.5->-r requirements.txt (line 5)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.5->-r requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.5->-r requirements.txt (line 5)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.5->-r requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score==0.3.5->-r requirements.txt (line 5)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score==0.3.5->-r requirements.txt (line 5)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score==0.3.5->-r requirements.txt (line 5)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score==0.3.5->-r requirements.txt (line 5)) (3.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=3.0.0->bert-score==0.3.5->-r requirements.txt (line 5)) (2023.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert-score==0.3.5->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score==0.3.5->-r requirements.txt (line 5)) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert-score==0.3.5->-r requirements.txt (line 5)) (1.3.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434675 sha256=0f756392734ead3f6b117c9f5bb5f95b11840d7bd62ee5af4d9ee008daa2ef04\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/ab/82/f9667f6f884d272670a15382599a9c753a1dfdc83f7412e37d\n",
            "Successfully built nltk\n",
            "Installing collected packages: tokenizers, tabulate, razdel, pyter3, nltk, huggingface-hub, transformers, bert-score\n",
            "  Attempting uninstall: tabulate\n",
            "    Found existing installation: tabulate 0.8.10\n",
            "    Uninstalling tabulate-0.8.10:\n",
            "      Successfully uninstalled tabulate-0.8.10\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed bert-score-0.3.5 huggingface-hub-0.15.1 nltk-3.5 pyter3-0.3 razdel-0.5.0 tabulate-0.8.7 tokenizers-0.13.3 transformers-4.29.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Cloning into 'bleurt'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 134 (delta 0), reused 17 (delta 0), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (134/134), 31.28 MiB | 7.59 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/lora/examples/NLG/eval/GenerationEval/bleurt\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (1.10.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (2.12.0)\n",
            "Requirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (1.1.0)\n",
            "Collecting sentencepiece (from BLEURT==0.0.2)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->BLEURT==0.0.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->BLEURT==0.0.2) (2022.7.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->BLEURT==0.0.2) (0.1.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->BLEURT==0.0.2) (3.2.2)\n",
            "Building wheels for collected packages: BLEURT\n",
            "  Building wheel for BLEURT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for BLEURT: filename=BLEURT-0.0.2-py3-none-any.whl size=16456764 sha256=bd8c9344b36003fc83b002f2306234f746c1003ba6943b3fe84aa101572c50cf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d_y9o7k7/wheels/c1/e2/57/89889ede6c030dbd8e5d2fb4c865fa9aa3d0883fc68a3aa6aa\n",
            "Successfully built BLEURT\n",
            "Installing collected packages: sentencepiece, BLEURT\n",
            "Successfully installed BLEURT-0.0.2 sentencepiece-0.1.99\n",
            "--2023-06-03 15:13:33--  https://storage.googleapis.com/bleurt-oss/bleurt-base-128.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.119.128, 108.177.126.128, 108.177.127.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.119.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 405489453 (387M) [application/zip]\n",
            "Saving to: ‘bleurt-base-128.zip’\n",
            "\n",
            "bleurt-base-128.zip 100%[===================>] 386.70M  37.3MB/s    in 12s     \n",
            "\n",
            "2023-06-03 15:13:45 (33.0 MB/s) - ‘bleurt-base-128.zip’ saved [405489453/405489453]\n",
            "\n",
            "Archive:  bleurt-base-128.zip\n",
            "   creating: bleurt-base-128/\n",
            "  inflating: bleurt-base-128/vocab.txt  \n",
            "  inflating: bleurt-base-128/bert_config.json  \n",
            "   creating: bleurt-base-128/variables/\n",
            "  inflating: bleurt-base-128/variables/variables.index  \n",
            "  inflating: bleurt-base-128/variables/variables.data-00000-of-00001  \n",
            "  inflating: bleurt-base-128/bleurt_config.json  \n",
            "  inflating: bleurt-base-128/saved_model.pb  \n",
            "--2023-06-03 15:13:49--  https://www.cs.cmu.edu/~alavie/METEOR/download/meteor-1.5.tar.gz\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 223646468 (213M) [application/x-gzip]\n",
            "Saving to: ‘meteor-1.5.tar.gz’\n",
            "\n",
            "meteor-1.5.tar.gz   100%[===================>] 213.29M  1.36MB/s    in 3m 27s  \n",
            "\n",
            "2023-06-03 15:17:17 (1.03 MB/s) - ‘meteor-1.5.tar.gz’ saved [223646468/223646468]\n",
            "\n",
            "meteor-1.5/\n",
            "meteor-1.5/data/\n",
            "meteor-1.5/data/paraphrase-es.gz\n",
            "meteor-1.5/data/paraphrase-cz.gz\n",
            "meteor-1.5/data/paraphrase-fr.gz\n",
            "meteor-1.5/data/paraphrase-en.gz\n",
            "meteor-1.5/data/paraphrase-de.gz\n",
            "meteor-1.5/data/paraphrase-ru.gz\n",
            "meteor-1.5/xray/\n",
            "meteor-1.5/xray/visualize_alignments.py\n",
            "meteor-1.5/xray/template/\n",
            "meteor-1.5/xray/template/score.tex\n",
            "meteor-1.5/xray/xray.py\n",
            "meteor-1.5/xray/MeteorAlignment.py\n",
            "meteor-1.5/xray/Generation.py\n",
            "meteor-1.5/scripts/\n",
            "meteor-1.5/scripts/sgmlize.py\n",
            "meteor-1.5/scripts/meteor_shower.py\n",
            "meteor-1.5/scripts/unroll_wmt_ranks.py\n",
            "meteor-1.5/scripts/meteorToMosesAlign.py\n",
            "meteor-1.5/scripts/delete_stray_matches.py\n",
            "meteor-1.5/scripts/bleu.py\n",
            "meteor-1.5/scripts/wmt_bleu.py\n",
            "meteor-1.5/scripts/tc_train_set.py\n",
            "meteor-1.5/scripts/wmt_ter.py\n",
            "meteor-1.5/scripts/freq.py\n",
            "meteor-1.5/scripts/filter_merge_rank_set.py\n",
            "meteor-1.5/scripts/wmt-eval.sh\n",
            "meteor-1.5/scripts/new_language.py\n",
            "meteor-1.5/scripts/tercom-0.8.0.jar\n",
            "meteor-1.5/scripts/ter.py\n",
            "meteor-1.5/scripts/agg.py\n",
            "meteor-1.5/scripts/build_wordnet_files.py\n",
            "meteor-1.5/scripts/combine_segcor_trainset.py\n",
            "meteor-1.5/scripts/rankconsist.py\n",
            "meteor-1.5/scripts/wmt_fmt.py\n",
            "meteor-1.5/scripts/mteval-v13m.pl\n",
            "meteor-1.5/README.html\n",
            "meteor-1.5/INSTALL\n",
            "meteor-1.5/resources/\n",
            "meteor-1.5/resources/stem/\n",
            "meteor-1.5/resources/stem/arabic-buckwalter-reduced.gz\n",
            "meteor-1.5/resources/nonbreaking/\n",
            "meteor-1.5/resources/nonbreaking/english.prefixes\n",
            "meteor-1.5/resources/nonbreaking/french.prefixes\n",
            "meteor-1.5/resources/nonbreaking/russian.prefixes\n",
            "meteor-1.5/resources/nonbreaking/german.prefixes\n",
            "meteor-1.5/resources/nonbreaking/spanish.prefixes\n",
            "meteor-1.5/resources/nonbreaking/czech.prefixes\n",
            "meteor-1.5/resources/synonym/\n",
            "meteor-1.5/resources/synonym/COPYING.WORDNET\n",
            "meteor-1.5/resources/synonym/english.synsets\n",
            "meteor-1.5/resources/synonym/english.exceptions\n",
            "meteor-1.5/resources/synonym/english.relations\n",
            "meteor-1.5/resources/function/\n",
            "meteor-1.5/resources/function/french.words\n",
            "meteor-1.5/resources/function/russian.words\n",
            "meteor-1.5/resources/function/romanian.words\n",
            "meteor-1.5/resources/function/dutch.words\n",
            "meteor-1.5/resources/function/arabic-buckwalter-reduced.words\n",
            "meteor-1.5/resources/function/portuguese.words\n",
            "meteor-1.5/resources/function/turkish.words\n",
            "meteor-1.5/resources/function/norwegian.words\n",
            "meteor-1.5/resources/function/danish.words\n",
            "meteor-1.5/resources/function/swedish.words\n",
            "meteor-1.5/resources/function/other.words\n",
            "meteor-1.5/resources/function/italian.words\n",
            "meteor-1.5/resources/function/english.words\n",
            "meteor-1.5/resources/function/spanish.words\n",
            "meteor-1.5/resources/function/czech.words\n",
            "meteor-1.5/resources/function/hungarian.words\n",
            "meteor-1.5/resources/function/german.words\n",
            "meteor-1.5/resources/function/finnish.words\n",
            "meteor-1.5/COPYING\n",
            "meteor-1.5/mt-diff/\n",
            "meteor-1.5/mt-diff/mt-diff.py\n",
            "meteor-1.5/mt-diff/files/\n",
            "meteor-1.5/mt-diff/files/mteval-v13m.pl\n",
            "meteor-1.5/example/\n",
            "meteor-1.5/example/xray/\n",
            "meteor-1.5/example/xray/reference\n",
            "meteor-1.5/example/xray/system1.hyp\n",
            "meteor-1.5/example/xray/system2.hyp\n",
            "meteor-1.5/example/tune/\n",
            "meteor-1.5/example/tune/example.score.in\n",
            "meteor-1.5/example/tune/example.eval.in\n",
            "meteor-1.5/example/tune/example.score.out\n",
            "meteor-1.5/example/tune/example.eval.out\n",
            "meteor-1.5/example/meteor-seg.scr\n",
            "meteor-1.5/example/meteor-sys.scr\n",
            "meteor-1.5/example/train/\n",
            "meteor-1.5/example/train/fr-en.sys2\n",
            "meteor-1.5/example/train/fr-en.sys1\n",
            "meteor-1.5/example/train/fr-en.ref\n",
            "meteor-1.5/example/train/fr-en.rank\n",
            "meteor-1.5/example/ref.sgm\n",
            "meteor-1.5/example/test.sgm\n",
            "meteor-1.5/example/meteor-doc.scr\n",
            "meteor-1.5/build.xml\n",
            "meteor-1.5/src/\n",
            "meteor-1.5/src/Trainer.java\n",
            "meteor-1.5/src/FilterParaphrase.java\n",
            "meteor-1.5/src/Meteor.java\n",
            "meteor-1.5/src/SGMtoPlaintext.java\n",
            "meteor-1.5/src/org/\n",
            "meteor-1.5/src/org/tartarus/\n",
            "meteor-1.5/src/org/tartarus/snowball/\n",
            "meteor-1.5/src/org/tartarus/snowball/Among.java\n",
            "meteor-1.5/src/org/tartarus/snowball/SnowballStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/germanStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/russianStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/swedishStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/hungarianStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/turkishStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/romanianStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/dutchStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/danishStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/portugueseStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/englishStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/finnishStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/porterStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/italianStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/norwegianStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/spanishStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/ext/frenchStemmer.java\n",
            "meteor-1.5/src/org/tartarus/snowball/SnowballProgram.java\n",
            "meteor-1.5/src/Parex.java\n",
            "meteor-1.5/src/edu/\n",
            "meteor-1.5/src/edu/cmu/\n",
            "meteor-1.5/src/edu/cmu/meteor/\n",
            "meteor-1.5/src/edu/cmu/meteor/scorer/\n",
            "meteor-1.5/src/edu/cmu/meteor/scorer/MeteorScorer.java\n",
            "meteor-1.5/src/edu/cmu/meteor/scorer/MeteorStats.java\n",
            "meteor-1.5/src/edu/cmu/meteor/scorer/MeteorConfiguration.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/ParaphraseTransducer.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/SynonymDictionary.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/ExactMatcher.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/ParaphraseMatcher.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/Alignment.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/Match.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/LookupTableStemmer.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/Stage.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/SnowballStemmerWrapper.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/StemMatcher.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/Aligner.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/PartialAlignment.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/Stemmer.java\n",
            "meteor-1.5/src/edu/cmu/meteor/aligner/SynonymMatcher.java\n",
            "meteor-1.5/src/edu/cmu/meteor/util/\n",
            "meteor-1.5/src/edu/cmu/meteor/util/Constants.java\n",
            "meteor-1.5/src/edu/cmu/meteor/util/Normalizer.java\n",
            "meteor-1.5/src/edu/cmu/meteor/util/SGMData.java\n",
            "meteor-1.5/src/edu/cmu/parex/\n",
            "meteor-1.5/src/edu/cmu/parex/PhraseTable.java\n",
            "meteor-1.5/src/edu/cmu/parex/ParaphraseExtractor.java\n",
            "meteor-1.5/src/edu/cmu/parex/Paraphrase.java\n",
            "meteor-1.5/src/Matcher.java\n",
            "meteor-1.5/src/Stemmer.java\n",
            "meteor-1.5/meteor-1.5.jar\n",
            "script complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/lora/')\n",
        "sys.path.append('/content/lora/examples/NLG/src')"
      ],
      "metadata": {
        "id": "D8CkfRkA5HOp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!torchrun --nproc_per_node=1 /content/lora/examples/NLG/src/gpt2_ft.py \\\n",
        "    --train_data /content/lora/examples/NLG/data/e2e/train.jsonl \\\n",
        "    --valid_data /content/lora/examples/NLG/data/e2e/valid.jsonl \\\n",
        "    --train_batch_size 8 \\\n",
        "    --grad_acc 1 \\\n",
        "    --valid_batch_size 4 \\\n",
        "    --seq_len 512 \\\n",
        "    --model_card gpt2.md \\\n",
        "    --init_checkpoint ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin \\\n",
        "    --platform local \\\n",
        "    --clip 0.0 \\\n",
        "    --lr 0.0002 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --correct_bias \\\n",
        "    --adam_beta2 0.999 \\\n",
        "    --scheduler linear \\\n",
        "    --warmup_step 500 \\\n",
        "    --max_epoch 5 \\\n",
        "    --save_interval 1000 \\\n",
        "    --lora_dim 4 \\\n",
        "    --lora_alpha 32 \\\n",
        "    --lora_dropout 0.1 \\\n",
        "    --label_smooth 0.1 \\\n",
        "    --work_dir ./lora/trained_models/GPT2_M/e2e \\\n",
        "    --random_seed 110"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF8p0wGr49nU",
        "outputId": "fdc08e09-73c0-4a49-8fa4-44d715364ba0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "myrank: 0 local_rank: 0 device_count: 1 world_size: 1\n",
            "====================================================================================================\n",
            "        - platform : local\n",
            "        - local_rank : 0\n",
            "        - rank : 0\n",
            "        - device : cuda:0\n",
            "        - world_size : 1\n",
            "        - random_seed : 110\n",
            "        - lr : 0.0002\n",
            "        - weight_decay : 0.01\n",
            "        - correct_bias : True\n",
            "        - adam_epislon : 1e-06\n",
            "        - no_decay_bias : False\n",
            "        - adam_beta1 : 0.9\n",
            "        - adam_beta2 : 0.999\n",
            "        - scheduler : linear\n",
            "        - max_step : None\n",
            "        - max_epoch : 5\n",
            "        - warmup_step : 500\n",
            "        - i_steps : 0\n",
            "        - i_lrs : 0.00025\n",
            "        - train_data : /content/lora/examples/NLG/data/e2e/train.jsonl\n",
            "        - valid_data : /content/lora/examples/NLG/data/e2e/valid.jsonl\n",
            "        - train_batch_size : 8\n",
            "        - valid_batch_size : 4\n",
            "        - grad_acc : 1\n",
            "        - clip : 0.0\n",
            "        - seq_len : 512\n",
            "        - model_card : gpt2.md\n",
            "        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin\n",
            "        - fp16 : False\n",
            "        - log_interval : 100\n",
            "        - eval_interval : 2000\n",
            "        - save_interval : 1000\n",
            "        - work_dir : ./trained_models/GPT2_M/e2e\n",
            "        - lora_dim : 4\n",
            "        - lora_alpha : 32\n",
            "        - obj : clm\n",
            "        - lora_dropout : 0.1\n",
            "        - label_smooth : 0.1\n",
            "        - roll_interval : -1\n",
            "        - roll_lr : 1e-05\n",
            "        - roll_step : 100\n",
            "        - eval_epoch : 1\n",
            "        - dist : <module 'torch.distributed' from '/usr/local/lib/python3.10/dist-packages/torch/distributed/__init__.py'>\n",
            "====================================================================================================\n",
            "Experiment dir : ./trained_models/GPT2_M/e2e\n",
            "loading model pretrained weight.\n",
            "set max_step: 26290\n",
            "start to train the model................ 1\n",
            "/content/lora/examples/NLG/src/optimizer.py:117: UserWarning: This overload of addcdiv_ is deprecated:\n",
            "\taddcdiv_(Number value, Tensor tensor1, Tensor tensor2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddcdiv_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n",
            "  p.data.addcdiv_(-step_size, exp_avg, denom)\n",
            "| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 663.74 | loss  5.07 | avg loss  5.52 | ppl 250.77\n",
            "| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 584.15 | loss  3.21 | avg loss  3.70 | ppl 40.55\n",
            "| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 584.00 | loss  2.98 | avg loss  3.08 | ppl 21.72\n",
            "| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 583.98 | loss  3.12 | avg loss  2.98 | ppl 19.63\n",
            "| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 583.97 | loss  2.84 | avg loss  2.89 | ppl 18.02\n",
            "| epoch   1 step      600 |    600 batches | lr 0.000199 | ms/batch 583.94 | loss  2.77 | avg loss  2.83 | ppl 16.94\n",
            "| epoch   1 step      700 |    700 batches | lr 0.000198 | ms/batch 583.94 | loss  2.89 | avg loss  2.80 | ppl 16.36\n",
            "| epoch   1 step      800 |    800 batches | lr 0.000198 | ms/batch 584.08 | loss  2.48 | avg loss  2.76 | ppl 15.74\n",
            "| epoch   1 step      900 |    900 batches | lr 0.000197 | ms/batch 583.90 | loss  2.51 | avg loss  2.75 | ppl 15.59\n",
            "| epoch   1 step     1000 |   1000 batches | lr 0.000196 | ms/batch 583.88 | loss  3.19 | avg loss  2.77 | ppl 15.95\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.1000.pt\n",
            "| epoch   1 step     1100 |   1100 batches | lr 0.000195 | ms/batch 583.97 | loss  2.84 | avg loss  2.76 | ppl 15.87\n",
            "| epoch   1 step     1200 |   1200 batches | lr 0.000195 | ms/batch 583.87 | loss  2.59 | avg loss  2.76 | ppl 15.78\n",
            "| epoch   1 step     1300 |   1300 batches | lr 0.000194 | ms/batch 583.70 | loss  2.62 | avg loss  2.72 | ppl 15.17\n",
            "| epoch   1 step     1400 |   1400 batches | lr 0.000193 | ms/batch 583.91 | loss  2.70 | avg loss  2.72 | ppl 15.22\n",
            "| epoch   1 step     1500 |   1500 batches | lr 0.000192 | ms/batch 584.15 | loss  2.67 | avg loss  2.73 | ppl 15.33\n",
            "| epoch   1 step     1600 |   1600 batches | lr 0.000191 | ms/batch 583.87 | loss  2.68 | avg loss  2.68 | ppl 14.64\n",
            "| epoch   1 step     1700 |   1700 batches | lr 0.000191 | ms/batch 584.03 | loss  2.57 | avg loss  2.70 | ppl 14.85\n",
            "| epoch   1 step     1800 |   1800 batches | lr 0.00019 | ms/batch 583.93 | loss  2.54 | avg loss  2.69 | ppl 14.72\n",
            "| epoch   1 step     1900 |   1900 batches | lr 0.000189 | ms/batch 583.85 | loss  2.70 | avg loss  2.69 | ppl 14.71\n",
            "| epoch   1 step     2000 |   2000 batches | lr 0.000188 | ms/batch 583.93 | loss  2.53 | avg loss  2.67 | ppl 14.50\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.2000.pt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "eval samples: 0 loss: tensor(1.3488, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.9342, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(1.3913, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.1998, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9218, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(0.7304, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.3245, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.8112, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.1845, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.7483, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(1.1870, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.4888, device='cuda:0')\n",
            "average loss 1.3115965620397705\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   1 at step     2000 | time: 159.03s | valid loss  1.31 | valid ppl  3.71 | best ppl  3.71 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 step     2100 |   2100 batches | lr 0.000188 | ms/batch 2174.26 | loss  2.69 | avg loss  2.68 | ppl 14.55\n",
            "| epoch   1 step     2200 |   2200 batches | lr 0.000187 | ms/batch 583.83 | loss  2.93 | avg loss  2.66 | ppl 14.33\n",
            "| epoch   1 step     2300 |   2300 batches | lr 0.000186 | ms/batch 583.78 | loss  2.69 | avg loss  2.66 | ppl 14.25\n",
            "| epoch   1 step     2400 |   2400 batches | lr 0.000185 | ms/batch 583.96 | loss  2.42 | avg loss  2.66 | ppl 14.35\n",
            "| epoch   1 step     2500 |   2500 batches | lr 0.000184 | ms/batch 583.87 | loss  2.83 | avg loss  2.68 | ppl 14.55\n",
            "| epoch   1 step     2600 |   2600 batches | lr 0.000184 | ms/batch 583.93 | loss  2.80 | avg loss  2.66 | ppl 14.29\n",
            "| epoch   1 step     2700 |   2700 batches | lr 0.000183 | ms/batch 584.00 | loss  2.69 | avg loss  2.65 | ppl 14.15\n",
            "| epoch   1 step     2800 |   2800 batches | lr 0.000182 | ms/batch 583.79 | loss  2.36 | avg loss  2.68 | ppl 14.62\n",
            "| epoch   1 step     2900 |   2900 batches | lr 0.000181 | ms/batch 583.80 | loss  2.85 | avg loss  2.67 | ppl 14.41\n",
            "| epoch   1 step     3000 |   3000 batches | lr 0.000181 | ms/batch 583.89 | loss  2.68 | avg loss  2.63 | ppl 13.92\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.3000.pt\n",
            "| epoch   1 step     3100 |   3100 batches | lr 0.00018 | ms/batch 584.02 | loss  2.90 | avg loss  2.63 | ppl 13.88\n",
            "| epoch   1 step     3200 |   3200 batches | lr 0.000179 | ms/batch 583.89 | loss  2.72 | avg loss  2.67 | ppl 14.41\n",
            "| epoch   1 step     3300 |   3300 batches | lr 0.000178 | ms/batch 584.01 | loss  3.03 | avg loss  2.63 | ppl 13.82\n",
            "| epoch   1 step     3400 |   3400 batches | lr 0.000178 | ms/batch 583.92 | loss  2.56 | avg loss  2.62 | ppl 13.78\n",
            "| epoch   1 step     3500 |   3500 batches | lr 0.000177 | ms/batch 584.09 | loss  2.39 | avg loss  2.66 | ppl 14.24\n",
            "| epoch   1 step     3600 |   3600 batches | lr 0.000176 | ms/batch 583.88 | loss  2.52 | avg loss  2.66 | ppl 14.28\n",
            "| epoch   1 step     3700 |   3700 batches | lr 0.000175 | ms/batch 583.99 | loss  2.59 | avg loss  2.63 | ppl 13.88\n",
            "| epoch   1 step     3800 |   3800 batches | lr 0.000174 | ms/batch 583.81 | loss  2.59 | avg loss  2.58 | ppl 13.24\n",
            "| epoch   1 step     3900 |   3900 batches | lr 0.000174 | ms/batch 583.85 | loss  2.34 | avg loss  2.66 | ppl 14.33\n",
            "| epoch   1 step     4000 |   4000 batches | lr 0.000173 | ms/batch 583.76 | loss  2.65 | avg loss  2.66 | ppl 14.26\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.4000.pt\n",
            "eval samples: 0 loss: tensor(1.3099, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.9408, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(1.3588, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.1679, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9471, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(0.6715, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.2952, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.7819, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.1910, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.7893, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(1.0995, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.4696, device='cuda:0')\n",
            "average loss 1.2786463848123812\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   2 at step     4000 | time: 159.05s | valid loss  1.28 | valid ppl  3.59 | best ppl  3.59 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 step     4100 |   4100 batches | lr 0.000172 | ms/batch 2174.35 | loss  2.72 | avg loss  2.61 | ppl 13.59\n",
            "| epoch   1 step     4200 |   4200 batches | lr 0.000171 | ms/batch 583.87 | loss  2.68 | avg loss  2.62 | ppl 13.71\n",
            "| epoch   1 step     4300 |   4300 batches | lr 0.000171 | ms/batch 584.01 | loss  2.43 | avg loss  2.57 | ppl 13.10\n",
            "| epoch   1 step     4400 |   4400 batches | lr 0.00017 | ms/batch 583.91 | loss  2.65 | avg loss  2.61 | ppl 13.55\n",
            "| epoch   1 step     4500 |   4500 batches | lr 0.000169 | ms/batch 583.91 | loss  2.78 | avg loss  2.63 | ppl 13.85\n",
            "| epoch   1 step     4600 |   4600 batches | lr 0.000168 | ms/batch 583.92 | loss  2.58 | avg loss  2.63 | ppl 13.85\n",
            "| epoch   1 step     4700 |   4700 batches | lr 0.000167 | ms/batch 583.89 | loss  2.38 | avg loss  2.62 | ppl 13.78\n",
            "| epoch   1 step     4800 |   4800 batches | lr 0.000167 | ms/batch 584.19 | loss  2.82 | avg loss  2.59 | ppl 13.36\n",
            "| epoch   1 step     4900 |   4900 batches | lr 0.000166 | ms/batch 583.86 | loss  2.63 | avg loss  2.61 | ppl 13.56\n",
            "| epoch   1 step     5000 |   5000 batches | lr 0.000165 | ms/batch 583.65 | loss  2.37 | avg loss  2.58 | ppl 13.22\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.5000.pt\n",
            "| epoch   1 step     5100 |   5100 batches | lr 0.000164 | ms/batch 583.94 | loss  2.67 | avg loss  2.62 | ppl 13.69\n",
            "| epoch   1 step     5200 |   5200 batches | lr 0.000164 | ms/batch 584.00 | loss  2.78 | avg loss  2.60 | ppl 13.50\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.5258.pt\n",
            "start to train the model................ 2\n",
            "| epoch   2 step     5300 |     42 batches | lr 0.000163 | ms/batch 245.67 | loss  2.44 | avg loss  2.54 | ppl 12.71\n",
            "| epoch   2 step     5400 |    142 batches | lr 0.000162 | ms/batch 583.80 | loss  2.52 | avg loss  2.61 | ppl 13.55\n",
            "| epoch   2 step     5500 |    242 batches | lr 0.000161 | ms/batch 583.97 | loss  2.61 | avg loss  2.59 | ppl 13.34\n",
            "| epoch   2 step     5600 |    342 batches | lr 0.00016 | ms/batch 584.12 | loss  2.22 | avg loss  2.61 | ppl 13.55\n",
            "| epoch   2 step     5700 |    442 batches | lr 0.00016 | ms/batch 583.85 | loss  2.54 | avg loss  2.60 | ppl 13.48\n",
            "| epoch   2 step     5800 |    542 batches | lr 0.000159 | ms/batch 584.05 | loss  2.61 | avg loss  2.57 | ppl 13.07\n",
            "| epoch   2 step     5900 |    642 batches | lr 0.000158 | ms/batch 583.90 | loss  2.67 | avg loss  2.60 | ppl 13.45\n",
            "| epoch   2 step     6000 |    742 batches | lr 0.000157 | ms/batch 583.85 | loss  2.31 | avg loss  2.56 | ppl 12.95\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.6000.pt\n",
            "eval samples: 0 loss: tensor(1.2663, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.9088, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(1.2435, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.1694, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.8977, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(0.6335, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.2861, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.7480, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.1763, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.7816, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(1.0711, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.4703, device='cuda:0')\n",
            "average loss 1.2385323187974218\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   3 at step     6000 | time: 159.23s | valid loss  1.24 | valid ppl  3.45 | best ppl  3.45 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 step     6100 |    842 batches | lr 0.000157 | ms/batch 2176.26 | loss  2.34 | avg loss  2.58 | ppl 13.24\n",
            "| epoch   2 step     6200 |    942 batches | lr 0.000156 | ms/batch 583.85 | loss  2.42 | avg loss  2.59 | ppl 13.27\n",
            "| epoch   2 step     6300 |   1042 batches | lr 0.000155 | ms/batch 583.98 | loss  2.80 | avg loss  2.58 | ppl 13.25\n",
            "| epoch   2 step     6400 |   1142 batches | lr 0.000154 | ms/batch 583.89 | loss  2.37 | avg loss  2.58 | ppl 13.19\n",
            "| epoch   2 step     6500 |   1242 batches | lr 0.000153 | ms/batch 583.94 | loss  2.42 | avg loss  2.60 | ppl 13.50\n",
            "| epoch   2 step     6600 |   1342 batches | lr 0.000153 | ms/batch 584.05 | loss  2.85 | avg loss  2.57 | ppl 13.12\n",
            "| epoch   2 step     6700 |   1442 batches | lr 0.000152 | ms/batch 583.91 | loss  2.54 | avg loss  2.58 | ppl 13.20\n",
            "| epoch   2 step     6800 |   1542 batches | lr 0.000151 | ms/batch 584.03 | loss  2.57 | avg loss  2.59 | ppl 13.36\n",
            "| epoch   2 step     6900 |   1642 batches | lr 0.00015 | ms/batch 583.87 | loss  2.52 | avg loss  2.53 | ppl 12.61\n",
            "| epoch   2 step     7000 |   1742 batches | lr 0.00015 | ms/batch 584.17 | loss  2.72 | avg loss  2.59 | ppl 13.27\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.7000.pt\n",
            "| epoch   2 step     7100 |   1842 batches | lr 0.000149 | ms/batch 583.99 | loss  2.71 | avg loss  2.58 | ppl 13.14\n",
            "| epoch   2 step     7200 |   1942 batches | lr 0.000148 | ms/batch 583.96 | loss  2.62 | avg loss  2.57 | ppl 13.12\n",
            "| epoch   2 step     7300 |   2042 batches | lr 0.000147 | ms/batch 583.78 | loss  2.44 | avg loss  2.55 | ppl 12.79\n",
            "| epoch   2 step     7400 |   2142 batches | lr 0.000146 | ms/batch 583.80 | loss  2.92 | avg loss  2.55 | ppl 12.76\n",
            "| epoch   2 step     7500 |   2242 batches | lr 0.000146 | ms/batch 583.70 | loss  2.70 | avg loss  2.59 | ppl 13.38\n",
            "| epoch   2 step     7600 |   2342 batches | lr 0.000145 | ms/batch 583.98 | loss  2.36 | avg loss  2.55 | ppl 12.84\n",
            "| epoch   2 step     7700 |   2442 batches | lr 0.000144 | ms/batch 583.85 | loss  2.73 | avg loss  2.55 | ppl 12.84\n",
            "| epoch   2 step     7800 |   2542 batches | lr 0.000143 | ms/batch 583.70 | loss  2.58 | avg loss  2.54 | ppl 12.72\n",
            "| epoch   2 step     7900 |   2642 batches | lr 0.000143 | ms/batch 583.75 | loss  2.86 | avg loss  2.58 | ppl 13.17\n",
            "| epoch   2 step     8000 |   2742 batches | lr 0.000142 | ms/batch 583.78 | loss  2.62 | avg loss  2.60 | ppl 13.40\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.8000.pt\n",
            "eval samples: 0 loss: tensor(1.2372, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.9090, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(1.2485, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.1387, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9059, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(0.6444, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.1865, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.7190, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.1513, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.7459, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(1.0361, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.4348, device='cuda:0')\n",
            "average loss 1.2212107114697972\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   4 at step     8000 | time: 159.03s | valid loss  1.22 | valid ppl  3.39 | best ppl  3.39 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 step     8100 |   2842 batches | lr 0.000141 | ms/batch 2174.27 | loss  2.41 | avg loss  2.55 | ppl 12.85\n",
            "| epoch   2 step     8200 |   2942 batches | lr 0.00014 | ms/batch 583.76 | loss  2.62 | avg loss  2.61 | ppl 13.56\n",
            "| epoch   2 step     8300 |   3042 batches | lr 0.00014 | ms/batch 583.80 | loss  2.45 | avg loss  2.57 | ppl 13.11\n",
            "| epoch   2 step     8400 |   3142 batches | lr 0.000139 | ms/batch 584.03 | loss  2.58 | avg loss  2.56 | ppl 12.93\n",
            "| epoch   2 step     8500 |   3242 batches | lr 0.000138 | ms/batch 583.74 | loss  2.54 | avg loss  2.56 | ppl 12.97\n",
            "| epoch   2 step     8600 |   3342 batches | lr 0.000137 | ms/batch 583.91 | loss  2.54 | avg loss  2.54 | ppl 12.74\n",
            "| epoch   2 step     8700 |   3442 batches | lr 0.000136 | ms/batch 583.88 | loss  2.90 | avg loss  2.55 | ppl 12.81\n",
            "| epoch   2 step     8800 |   3542 batches | lr 0.000136 | ms/batch 583.75 | loss  2.43 | avg loss  2.55 | ppl 12.81\n",
            "| epoch   2 step     8900 |   3642 batches | lr 0.000135 | ms/batch 583.82 | loss  2.62 | avg loss  2.58 | ppl 13.13\n",
            "| epoch   2 step     9000 |   3742 batches | lr 0.000134 | ms/batch 583.88 | loss  2.55 | avg loss  2.56 | ppl 12.96\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.9000.pt\n",
            "| epoch   2 step     9100 |   3842 batches | lr 0.000133 | ms/batch 584.09 | loss  2.50 | avg loss  2.56 | ppl 12.89\n",
            "| epoch   2 step     9200 |   3942 batches | lr 0.000133 | ms/batch 583.93 | loss  2.69 | avg loss  2.56 | ppl 12.92\n",
            "| epoch   2 step     9300 |   4042 batches | lr 0.000132 | ms/batch 583.96 | loss  2.56 | avg loss  2.57 | ppl 13.10\n",
            "| epoch   2 step     9400 |   4142 batches | lr 0.000131 | ms/batch 583.88 | loss  2.60 | avg loss  2.56 | ppl 12.87\n",
            "| epoch   2 step     9500 |   4242 batches | lr 0.00013 | ms/batch 583.68 | loss  2.58 | avg loss  2.56 | ppl 12.87\n",
            "| epoch   2 step     9600 |   4342 batches | lr 0.000129 | ms/batch 583.89 | loss  2.92 | avg loss  2.56 | ppl 12.99\n",
            "| epoch   2 step     9700 |   4442 batches | lr 0.000129 | ms/batch 584.11 | loss  2.30 | avg loss  2.55 | ppl 12.75\n",
            "| epoch   2 step     9800 |   4542 batches | lr 0.000128 | ms/batch 584.17 | loss  2.56 | avg loss  2.55 | ppl 12.75\n",
            "| epoch   2 step     9900 |   4642 batches | lr 0.000127 | ms/batch 584.46 | loss  2.26 | avg loss  2.53 | ppl 12.51\n",
            "| epoch   2 step    10000 |   4742 batches | lr 0.000126 | ms/batch 583.96 | loss  2.65 | avg loss  2.57 | ppl 13.03\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.10000.pt\n",
            "eval samples: 0 loss: tensor(1.1959, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.8767, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(1.2672, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0960, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.8903, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(0.5650, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.1654, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.6889, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.1379, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.7630, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(0.9961, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.4826, device='cuda:0')\n",
            "average loss 1.1940214522948411\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   5 at step    10000 | time: 159.06s | valid loss  1.19 | valid ppl  3.30 | best ppl  3.30 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 step    10100 |   4842 batches | lr 0.000126 | ms/batch 2174.67 | loss  2.37 | avg loss  2.55 | ppl 12.83\n",
            "| epoch   2 step    10200 |   4942 batches | lr 0.000125 | ms/batch 583.83 | loss  2.45 | avg loss  2.56 | ppl 12.93\n",
            "| epoch   2 step    10300 |   5042 batches | lr 0.000124 | ms/batch 583.88 | loss  2.83 | avg loss  2.54 | ppl 12.70\n",
            "| epoch   2 step    10400 |   5142 batches | lr 0.000123 | ms/batch 583.90 | loss  2.71 | avg loss  2.54 | ppl 12.70\n",
            "| epoch   2 step    10500 |   5242 batches | lr 0.000122 | ms/batch 583.70 | loss  2.45 | avg loss  2.52 | ppl 12.46\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.10516.pt\n",
            "start to train the model................ 3\n",
            "| epoch   3 step    10600 |     84 batches | lr 0.000122 | ms/batch 490.92 | loss  2.20 | avg loss  2.53 | ppl 12.56\n",
            "| epoch   3 step    10700 |    184 batches | lr 0.000121 | ms/batch 583.77 | loss  2.63 | avg loss  2.55 | ppl 12.75\n",
            "| epoch   3 step    10800 |    284 batches | lr 0.00012 | ms/batch 583.70 | loss  2.76 | avg loss  2.53 | ppl 12.54\n",
            "| epoch   3 step    10900 |    384 batches | lr 0.000119 | ms/batch 583.80 | loss  2.83 | avg loss  2.53 | ppl 12.56\n",
            "| epoch   3 step    11000 |    484 batches | lr 0.000119 | ms/batch 583.70 | loss  2.65 | avg loss  2.53 | ppl 12.55\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.11000.pt\n",
            "| epoch   3 step    11100 |    584 batches | lr 0.000118 | ms/batch 583.83 | loss  2.56 | avg loss  2.59 | ppl 13.31\n",
            "| epoch   3 step    11200 |    684 batches | lr 0.000117 | ms/batch 584.18 | loss  2.58 | avg loss  2.54 | ppl 12.63\n",
            "| epoch   3 step    11300 |    784 batches | lr 0.000116 | ms/batch 583.87 | loss  2.62 | avg loss  2.55 | ppl 12.87\n",
            "| epoch   3 step    11400 |    884 batches | lr 0.000115 | ms/batch 583.98 | loss  2.56 | avg loss  2.54 | ppl 12.73\n",
            "| epoch   3 step    11500 |    984 batches | lr 0.000115 | ms/batch 583.66 | loss  2.28 | avg loss  2.55 | ppl 12.79\n",
            "| epoch   3 step    11600 |   1084 batches | lr 0.000114 | ms/batch 583.72 | loss  2.19 | avg loss  2.50 | ppl 12.22\n",
            "| epoch   3 step    11700 |   1184 batches | lr 0.000113 | ms/batch 583.99 | loss  2.36 | avg loss  2.52 | ppl 12.48\n",
            "| epoch   3 step    11800 |   1284 batches | lr 0.000112 | ms/batch 583.87 | loss  2.48 | avg loss  2.50 | ppl 12.15\n",
            "| epoch   3 step    11900 |   1384 batches | lr 0.000112 | ms/batch 583.81 | loss  2.42 | avg loss  2.51 | ppl 12.34\n",
            "| epoch   3 step    12000 |   1484 batches | lr 0.000111 | ms/batch 583.71 | loss  2.61 | avg loss  2.53 | ppl 12.53\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.12000.pt\n",
            "eval samples: 0 loss: tensor(1.2206, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.9033, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(1.2574, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.1268, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9291, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(0.5768, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.1954, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.6934, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.1493, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.7669, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(0.9658, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.5108, device='cuda:0')\n",
            "average loss 1.2139290378314176\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   6 at step    12000 | time: 159.11s | valid loss  1.21 | valid ppl  3.37 | best ppl  3.37 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   3 step    12100 |   1584 batches | lr 0.00011 | ms/batch 2175.07 | loss  2.50 | avg loss  2.54 | ppl 12.71\n",
            "| epoch   3 step    12200 |   1684 batches | lr 0.000109 | ms/batch 583.61 | loss  2.46 | avg loss  2.55 | ppl 12.84\n",
            "| epoch   3 step    12300 |   1784 batches | lr 0.000108 | ms/batch 583.65 | loss  2.47 | avg loss  2.54 | ppl 12.66\n",
            "| epoch   3 step    12400 |   1884 batches | lr 0.000108 | ms/batch 583.73 | loss  2.59 | avg loss  2.51 | ppl 12.26\n",
            "| epoch   3 step    12500 |   1984 batches | lr 0.000107 | ms/batch 583.83 | loss  2.78 | avg loss  2.53 | ppl 12.60\n",
            "| epoch   3 step    12600 |   2084 batches | lr 0.000106 | ms/batch 583.87 | loss  2.81 | avg loss  2.47 | ppl 11.88\n",
            "| epoch   3 step    12700 |   2184 batches | lr 0.000105 | ms/batch 583.73 | loss  2.35 | avg loss  2.53 | ppl 12.50\n",
            "| epoch   3 step    12800 |   2284 batches | lr 0.000105 | ms/batch 583.78 | loss  2.61 | avg loss  2.53 | ppl 12.57\n",
            "| epoch   3 step    12900 |   2384 batches | lr 0.000104 | ms/batch 583.80 | loss  2.43 | avg loss  2.53 | ppl 12.57\n",
            "| epoch   3 step    13000 |   2484 batches | lr 0.000103 | ms/batch 583.98 | loss  3.06 | avg loss  2.55 | ppl 12.78\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.13000.pt\n",
            "| epoch   3 step    13100 |   2584 batches | lr 0.000102 | ms/batch 583.86 | loss  2.35 | avg loss  2.50 | ppl 12.18\n",
            "| epoch   3 step    13200 |   2684 batches | lr 0.000102 | ms/batch 583.87 | loss  2.46 | avg loss  2.56 | ppl 12.89\n",
            "| epoch   3 step    13300 |   2784 batches | lr 0.000101 | ms/batch 583.65 | loss  2.45 | avg loss  2.51 | ppl 12.27\n",
            "| epoch   3 step    13400 |   2884 batches | lr 0.0001 | ms/batch 583.85 | loss  2.64 | avg loss  2.50 | ppl 12.23\n",
            "| epoch   3 step    13500 |   2984 batches | lr 9.92e-05 | ms/batch 583.98 | loss  2.96 | avg loss  2.55 | ppl 12.76\n",
            "| epoch   3 step    13600 |   3084 batches | lr 9.84e-05 | ms/batch 584.05 | loss  2.31 | avg loss  2.54 | ppl 12.62\n",
            "| epoch   3 step    13700 |   3184 batches | lr 9.76e-05 | ms/batch 583.90 | loss  2.74 | avg loss  2.57 | ppl 13.03\n",
            "| epoch   3 step    13800 |   3284 batches | lr 9.69e-05 | ms/batch 583.45 | loss  2.29 | avg loss  2.50 | ppl 12.17\n",
            "| epoch   3 step    13900 |   3384 batches | lr 9.61e-05 | ms/batch 583.73 | loss  2.92 | avg loss  2.55 | ppl 12.75\n",
            "| epoch   3 step    14000 |   3484 batches | lr 9.53e-05 | ms/batch 583.84 | loss  2.67 | avg loss  2.55 | ppl 12.78\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.14000.pt\n",
            "eval samples: 0 loss: tensor(1.1667, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.8804, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(1.2847, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.1187, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9158, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(0.5827, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.2015, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.7070, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.1271, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.7450, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(0.9733, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.5159, device='cuda:0')\n",
            "average loss 1.1963048857215741\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   7 at step    14000 | time: 159.13s | valid loss  1.20 | valid ppl  3.31 | best ppl  3.31 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   3 step    14100 |   3584 batches | lr 9.45e-05 | ms/batch 2175.16 | loss  2.56 | avg loss  2.48 | ppl 11.95\n",
            "| epoch   3 step    14200 |   3684 batches | lr 9.38e-05 | ms/batch 584.00 | loss  2.53 | avg loss  2.50 | ppl 12.17\n",
            "| epoch   3 step    14300 |   3784 batches | lr 9.3e-05 | ms/batch 583.84 | loss  2.74 | avg loss  2.50 | ppl 12.24\n",
            "| epoch   3 step    14400 |   3884 batches | lr 9.22e-05 | ms/batch 583.85 | loss  2.28 | avg loss  2.51 | ppl 12.31\n",
            "| epoch   3 step    14500 |   3984 batches | lr 9.14e-05 | ms/batch 583.89 | loss  2.37 | avg loss  2.53 | ppl 12.55\n",
            "| epoch   3 step    14600 |   4084 batches | lr 9.07e-05 | ms/batch 583.85 | loss  2.50 | avg loss  2.53 | ppl 12.61\n",
            "| epoch   3 step    14700 |   4184 batches | lr 8.99e-05 | ms/batch 583.93 | loss  2.51 | avg loss  2.50 | ppl 12.15\n",
            "| epoch   3 step    14800 |   4284 batches | lr 8.91e-05 | ms/batch 583.97 | loss  2.51 | avg loss  2.54 | ppl 12.68\n",
            "| epoch   3 step    14900 |   4384 batches | lr 8.83e-05 | ms/batch 583.78 | loss  2.48 | avg loss  2.51 | ppl 12.35\n",
            "| epoch   3 step    15000 |   4484 batches | lr 8.76e-05 | ms/batch 583.93 | loss  2.79 | avg loss  2.54 | ppl 12.72\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.15000.pt\n",
            "| epoch   3 step    15100 |   4584 batches | lr 8.68e-05 | ms/batch 583.82 | loss  2.35 | avg loss  2.51 | ppl 12.29\n",
            "| epoch   3 step    15200 |   4684 batches | lr 8.6e-05 | ms/batch 583.88 | loss  2.52 | avg loss  2.50 | ppl 12.24\n",
            "| epoch   3 step    15300 |   4784 batches | lr 8.52e-05 | ms/batch 583.98 | loss  2.49 | avg loss  2.54 | ppl 12.71\n",
            "| epoch   3 step    15400 |   4884 batches | lr 8.45e-05 | ms/batch 583.87 | loss  2.38 | avg loss  2.52 | ppl 12.44\n",
            "| epoch   3 step    15500 |   4984 batches | lr 8.37e-05 | ms/batch 583.95 | loss  2.41 | avg loss  2.52 | ppl 12.40\n",
            "| epoch   3 step    15600 |   5084 batches | lr 8.29e-05 | ms/batch 583.81 | loss  2.64 | avg loss  2.51 | ppl 12.28\n",
            "| epoch   3 step    15700 |   5184 batches | lr 8.21e-05 | ms/batch 583.99 | loss  2.47 | avg loss  2.49 | ppl 12.02\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.15774.pt\n",
            "start to train the model................ 4\n",
            "| epoch   4 step    15800 |     26 batches | lr 8.13e-05 | ms/batch 152.43 | loss  2.36 | avg loss  2.50 | ppl 12.14\n",
            "| epoch   4 step    15900 |    126 batches | lr 8.06e-05 | ms/batch 583.83 | loss  2.37 | avg loss  2.50 | ppl 12.18\n",
            "| epoch   4 step    16000 |    226 batches | lr 7.98e-05 | ms/batch 583.87 | loss  2.53 | avg loss  2.52 | ppl 12.44\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.16000.pt\n",
            "eval samples: 0 loss: tensor(1.1993, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.8827, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(1.2374, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.1243, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.9122, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(0.5526, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.1542, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.6525, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.1614, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.7549, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(0.9438, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.4752, device='cuda:0')\n",
            "average loss 1.1857772473603079\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   8 at step    16000 | time: 159.21s | valid loss  1.19 | valid ppl  3.27 | best ppl  3.27 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   4 step    16100 |    326 batches | lr 7.9e-05 | ms/batch 2175.97 | loss  2.74 | avg loss  2.49 | ppl 12.05\n",
            "| epoch   4 step    16200 |    426 batches | lr 7.82e-05 | ms/batch 583.88 | loss  2.46 | avg loss  2.50 | ppl 12.18\n",
            "| epoch   4 step    16300 |    526 batches | lr 7.75e-05 | ms/batch 584.02 | loss  2.86 | avg loss  2.49 | ppl 12.04\n",
            "| epoch   4 step    16400 |    626 batches | lr 7.67e-05 | ms/batch 583.74 | loss  2.91 | avg loss  2.50 | ppl 12.16\n",
            "| epoch   4 step    16500 |    726 batches | lr 7.59e-05 | ms/batch 583.85 | loss  2.75 | avg loss  2.53 | ppl 12.55\n",
            "| epoch   4 step    16600 |    826 batches | lr 7.51e-05 | ms/batch 583.84 | loss  2.69 | avg loss  2.50 | ppl 12.14\n",
            "| epoch   4 step    16700 |    926 batches | lr 7.44e-05 | ms/batch 583.83 | loss  2.70 | avg loss  2.51 | ppl 12.35\n",
            "| epoch   4 step    16800 |   1026 batches | lr 7.36e-05 | ms/batch 583.81 | loss  2.66 | avg loss  2.51 | ppl 12.27\n",
            "| epoch   4 step    16900 |   1126 batches | lr 7.28e-05 | ms/batch 583.74 | loss  2.20 | avg loss  2.48 | ppl 11.93\n",
            "| epoch   4 step    17000 |   1226 batches | lr 7.2e-05 | ms/batch 583.91 | loss  2.35 | avg loss  2.48 | ppl 11.89\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.17000.pt\n",
            "| epoch   4 step    17100 |   1326 batches | lr 7.13e-05 | ms/batch 583.70 | loss  2.62 | avg loss  2.53 | ppl 12.55\n",
            "| epoch   4 step    17200 |   1426 batches | lr 7.05e-05 | ms/batch 583.81 | loss  2.37 | avg loss  2.48 | ppl 11.91\n",
            "| epoch   4 step    17300 |   1526 batches | lr 6.97e-05 | ms/batch 583.85 | loss  2.36 | avg loss  2.51 | ppl 12.29\n",
            "| epoch   4 step    17400 |   1626 batches | lr 6.89e-05 | ms/batch 583.84 | loss  2.30 | avg loss  2.51 | ppl 12.32\n",
            "| epoch   4 step    17500 |   1726 batches | lr 6.82e-05 | ms/batch 583.93 | loss  2.69 | avg loss  2.52 | ppl 12.39\n",
            "| epoch   4 step    17600 |   1826 batches | lr 6.74e-05 | ms/batch 583.78 | loss  2.82 | avg loss  2.52 | ppl 12.44\n",
            "| epoch   4 step    17700 |   1926 batches | lr 6.66e-05 | ms/batch 583.83 | loss  2.39 | avg loss  2.49 | ppl 12.06\n",
            "| epoch   4 step    17800 |   2026 batches | lr 6.58e-05 | ms/batch 583.67 | loss  2.74 | avg loss  2.53 | ppl 12.54\n",
            "| epoch   4 step    17900 |   2126 batches | lr 6.51e-05 | ms/batch 583.70 | loss  2.49 | avg loss  2.49 | ppl 12.04\n",
            "| epoch   4 step    18000 |   2226 batches | lr 6.43e-05 | ms/batch 583.79 | loss  3.01 | avg loss  2.50 | ppl 12.16\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.18000.pt\n",
            "eval samples: 0 loss: tensor(1.1507, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.8833, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(1.2578, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0848, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.8957, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(0.5621, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.1626, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.6756, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.1230, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.7398, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(0.9545, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.4945, device='cuda:0')\n",
            "average loss 1.1829424180498678\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval   9 at step    18000 | time: 159.03s | valid loss  1.18 | valid ppl  3.26 | best ppl  3.26 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   4 step    18100 |   2326 batches | lr 6.35e-05 | ms/batch 2173.97 | loss  2.61 | avg loss  2.49 | ppl 12.12\n",
            "| epoch   4 step    18200 |   2426 batches | lr 6.27e-05 | ms/batch 583.65 | loss  2.38 | avg loss  2.50 | ppl 12.19\n",
            "| epoch   4 step    18300 |   2526 batches | lr 6.2e-05 | ms/batch 583.77 | loss  2.41 | avg loss  2.51 | ppl 12.29\n",
            "| epoch   4 step    18400 |   2626 batches | lr 6.12e-05 | ms/batch 583.82 | loss  2.62 | avg loss  2.49 | ppl 12.01\n",
            "| epoch   4 step    18500 |   2726 batches | lr 6.04e-05 | ms/batch 583.79 | loss  2.51 | avg loss  2.47 | ppl 11.85\n",
            "| epoch   4 step    18600 |   2826 batches | lr 5.96e-05 | ms/batch 583.59 | loss  2.28 | avg loss  2.52 | ppl 12.49\n",
            "| epoch   4 step    18700 |   2926 batches | lr 5.89e-05 | ms/batch 583.77 | loss  2.29 | avg loss  2.49 | ppl 12.02\n",
            "| epoch   4 step    18800 |   3026 batches | lr 5.81e-05 | ms/batch 583.87 | loss  2.29 | avg loss  2.51 | ppl 12.35\n",
            "| epoch   4 step    18900 |   3126 batches | lr 5.73e-05 | ms/batch 583.73 | loss  2.47 | avg loss  2.50 | ppl 12.20\n",
            "| epoch   4 step    19000 |   3226 batches | lr 5.65e-05 | ms/batch 583.74 | loss  2.80 | avg loss  2.50 | ppl 12.24\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.19000.pt\n",
            "| epoch   4 step    19100 |   3326 batches | lr 5.58e-05 | ms/batch 583.85 | loss  2.69 | avg loss  2.48 | ppl 12.00\n",
            "| epoch   4 step    19200 |   3426 batches | lr 5.5e-05 | ms/batch 584.12 | loss  2.58 | avg loss  2.48 | ppl 11.93\n",
            "| epoch   4 step    19300 |   3526 batches | lr 5.42e-05 | ms/batch 584.03 | loss  2.56 | avg loss  2.51 | ppl 12.35\n",
            "| epoch   4 step    19400 |   3626 batches | lr 5.34e-05 | ms/batch 583.99 | loss  2.65 | avg loss  2.49 | ppl 12.08\n",
            "| epoch   4 step    19500 |   3726 batches | lr 5.27e-05 | ms/batch 583.97 | loss  2.24 | avg loss  2.49 | ppl 12.06\n",
            "| epoch   4 step    19600 |   3826 batches | lr 5.19e-05 | ms/batch 584.10 | loss  2.63 | avg loss  2.50 | ppl 12.15\n",
            "| epoch   4 step    19700 |   3926 batches | lr 5.11e-05 | ms/batch 583.81 | loss  2.59 | avg loss  2.49 | ppl 12.06\n",
            "| epoch   4 step    19800 |   4026 batches | lr 5.03e-05 | ms/batch 583.77 | loss  2.50 | avg loss  2.50 | ppl 12.19\n",
            "| epoch   4 step    19900 |   4126 batches | lr 4.96e-05 | ms/batch 583.78 | loss  2.35 | avg loss  2.53 | ppl 12.61\n",
            "| epoch   4 step    20000 |   4226 batches | lr 4.88e-05 | ms/batch 583.84 | loss  2.40 | avg loss  2.50 | ppl 12.17\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.20000.pt\n",
            "eval samples: 0 loss: tensor(1.2032, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.8926, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(1.2436, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0859, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.8941, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(0.5596, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.1563, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.6712, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.1120, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.7358, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(0.9586, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.4656, device='cuda:0')\n",
            "average loss 1.1776583026839447\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  10 at step    20000 | time: 159.16s | valid loss  1.18 | valid ppl  3.25 | best ppl  3.25 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   4 step    20100 |   4326 batches | lr 4.8e-05 | ms/batch 2175.41 | loss  2.40 | avg loss  2.49 | ppl 12.05\n",
            "| epoch   4 step    20200 |   4426 batches | lr 4.72e-05 | ms/batch 583.80 | loss  2.52 | avg loss  2.50 | ppl 12.14\n",
            "| epoch   4 step    20300 |   4526 batches | lr 4.65e-05 | ms/batch 583.86 | loss  2.51 | avg loss  2.52 | ppl 12.43\n",
            "| epoch   4 step    20400 |   4626 batches | lr 4.57e-05 | ms/batch 583.84 | loss  2.44 | avg loss  2.49 | ppl 12.11\n",
            "| epoch   4 step    20500 |   4726 batches | lr 4.49e-05 | ms/batch 583.89 | loss  2.68 | avg loss  2.47 | ppl 11.88\n",
            "| epoch   4 step    20600 |   4826 batches | lr 4.41e-05 | ms/batch 583.75 | loss  2.65 | avg loss  2.52 | ppl 12.44\n",
            "| epoch   4 step    20700 |   4926 batches | lr 4.34e-05 | ms/batch 583.83 | loss  2.61 | avg loss  2.48 | ppl 11.98\n",
            "| epoch   4 step    20800 |   5026 batches | lr 4.26e-05 | ms/batch 583.54 | loss  2.30 | avg loss  2.52 | ppl 12.40\n",
            "| epoch   4 step    20900 |   5126 batches | lr 4.18e-05 | ms/batch 583.33 | loss  2.39 | avg loss  2.48 | ppl 11.97\n",
            "| epoch   4 step    21000 |   5226 batches | lr 4.1e-05 | ms/batch 583.41 | loss  2.29 | avg loss  2.48 | ppl 11.97\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.21000.pt\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.21032.pt\n",
            "start to train the model................ 5\n",
            "| epoch   5 step    21100 |     68 batches | lr 4.02e-05 | ms/batch 397.06 | loss  2.63 | avg loss  2.52 | ppl 12.37\n",
            "| epoch   5 step    21200 |    168 batches | lr 3.95e-05 | ms/batch 583.35 | loss  2.62 | avg loss  2.46 | ppl 11.65\n",
            "| epoch   5 step    21300 |    268 batches | lr 3.87e-05 | ms/batch 583.38 | loss  2.28 | avg loss  2.50 | ppl 12.20\n",
            "| epoch   5 step    21400 |    368 batches | lr 3.79e-05 | ms/batch 583.37 | loss  2.56 | avg loss  2.49 | ppl 12.06\n",
            "| epoch   5 step    21500 |    468 batches | lr 3.71e-05 | ms/batch 583.30 | loss  2.68 | avg loss  2.50 | ppl 12.18\n",
            "| epoch   5 step    21600 |    568 batches | lr 3.64e-05 | ms/batch 583.32 | loss  2.79 | avg loss  2.46 | ppl 11.68\n",
            "| epoch   5 step    21700 |    668 batches | lr 3.56e-05 | ms/batch 583.28 | loss  2.82 | avg loss  2.50 | ppl 12.15\n",
            "| epoch   5 step    21800 |    768 batches | lr 3.48e-05 | ms/batch 583.31 | loss  2.40 | avg loss  2.51 | ppl 12.33\n",
            "| epoch   5 step    21900 |    868 batches | lr 3.4e-05 | ms/batch 583.31 | loss  2.71 | avg loss  2.49 | ppl 12.09\n",
            "| epoch   5 step    22000 |    968 batches | lr 3.33e-05 | ms/batch 583.36 | loss  2.42 | avg loss  2.44 | ppl 11.46\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.22000.pt\n",
            "eval samples: 0 loss: tensor(1.1627, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.8813, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(1.2457, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0873, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.8758, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(0.5556, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.1528, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.6554, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.1370, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.7494, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(0.9134, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.4937, device='cuda:0')\n",
            "average loss 1.1714046616107225\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  11 at step    22000 | time: 158.94s | valid loss  1.17 | valid ppl  3.23 | best ppl  3.23 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   5 step    22100 |   1068 batches | lr 3.25e-05 | ms/batch 2172.80 | loss  2.59 | avg loss  2.47 | ppl 11.78\n",
            "| epoch   5 step    22200 |   1168 batches | lr 3.17e-05 | ms/batch 583.80 | loss  2.89 | avg loss  2.50 | ppl 12.13\n",
            "| epoch   5 step    22300 |   1268 batches | lr 3.09e-05 | ms/batch 583.34 | loss  2.31 | avg loss  2.50 | ppl 12.18\n",
            "| epoch   5 step    22400 |   1368 batches | lr 3.02e-05 | ms/batch 582.96 | loss  2.30 | avg loss  2.45 | ppl 11.57\n",
            "| epoch   5 step    22500 |   1468 batches | lr 2.94e-05 | ms/batch 583.11 | loss  2.52 | avg loss  2.49 | ppl 12.02\n",
            "| epoch   5 step    22600 |   1568 batches | lr 2.86e-05 | ms/batch 583.08 | loss  2.69 | avg loss  2.50 | ppl 12.19\n",
            "| epoch   5 step    22700 |   1668 batches | lr 2.78e-05 | ms/batch 583.06 | loss  2.38 | avg loss  2.48 | ppl 11.89\n",
            "| epoch   5 step    22800 |   1768 batches | lr 2.71e-05 | ms/batch 583.17 | loss  2.33 | avg loss  2.49 | ppl 12.08\n",
            "| epoch   5 step    22900 |   1868 batches | lr 2.63e-05 | ms/batch 583.14 | loss  2.87 | avg loss  2.53 | ppl 12.57\n",
            "| epoch   5 step    23000 |   1968 batches | lr 2.55e-05 | ms/batch 582.88 | loss  2.38 | avg loss  2.48 | ppl 11.89\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.23000.pt\n",
            "| epoch   5 step    23100 |   2068 batches | lr 2.47e-05 | ms/batch 582.92 | loss  2.57 | avg loss  2.51 | ppl 12.24\n",
            "| epoch   5 step    23200 |   2168 batches | lr 2.4e-05 | ms/batch 582.82 | loss  2.64 | avg loss  2.48 | ppl 11.93\n",
            "| epoch   5 step    23300 |   2268 batches | lr 2.32e-05 | ms/batch 583.11 | loss  2.34 | avg loss  2.46 | ppl 11.67\n",
            "| epoch   5 step    23400 |   2368 batches | lr 2.24e-05 | ms/batch 583.12 | loss  2.39 | avg loss  2.49 | ppl 12.04\n",
            "| epoch   5 step    23500 |   2468 batches | lr 2.16e-05 | ms/batch 583.06 | loss  2.35 | avg loss  2.46 | ppl 11.74\n",
            "| epoch   5 step    23600 |   2568 batches | lr 2.09e-05 | ms/batch 583.04 | loss  2.21 | avg loss  2.48 | ppl 11.91\n",
            "| epoch   5 step    23700 |   2668 batches | lr 2.01e-05 | ms/batch 583.13 | loss  2.31 | avg loss  2.45 | ppl 11.58\n",
            "| epoch   5 step    23800 |   2768 batches | lr 1.93e-05 | ms/batch 583.08 | loss  2.58 | avg loss  2.49 | ppl 12.09\n",
            "| epoch   5 step    23900 |   2868 batches | lr 1.85e-05 | ms/batch 582.88 | loss  2.53 | avg loss  2.46 | ppl 11.74\n",
            "| epoch   5 step    24000 |   2968 batches | lr 1.78e-05 | ms/batch 583.03 | loss  2.37 | avg loss  2.49 | ppl 12.03\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.24000.pt\n",
            "eval samples: 0 loss: tensor(1.1746, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.8785, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(1.2445, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.1005, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.8846, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(0.5544, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.1721, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.6615, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.1272, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.7379, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(0.9266, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.5191, device='cuda:0')\n",
            "average loss 1.1722285271660513\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  12 at step    24000 | time: 158.88s | valid loss  1.17 | valid ppl  3.23 | best ppl  3.23 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   5 step    24100 |   3068 batches | lr 1.7e-05 | ms/batch 2172.14 | loss  2.65 | avg loss  2.51 | ppl 12.33\n",
            "| epoch   5 step    24200 |   3168 batches | lr 1.62e-05 | ms/batch 583.08 | loss  2.64 | avg loss  2.44 | ppl 11.52\n",
            "| epoch   5 step    24300 |   3268 batches | lr 1.54e-05 | ms/batch 583.11 | loss  2.57 | avg loss  2.49 | ppl 12.09\n",
            "| epoch   5 step    24400 |   3368 batches | lr 1.47e-05 | ms/batch 583.16 | loss  2.61 | avg loss  2.48 | ppl 11.92\n",
            "| epoch   5 step    24500 |   3468 batches | lr 1.39e-05 | ms/batch 583.15 | loss  2.35 | avg loss  2.48 | ppl 11.89\n",
            "| epoch   5 step    24600 |   3568 batches | lr 1.31e-05 | ms/batch 583.23 | loss  2.55 | avg loss  2.47 | ppl 11.88\n",
            "| epoch   5 step    24700 |   3668 batches | lr 1.23e-05 | ms/batch 583.31 | loss  2.84 | avg loss  2.52 | ppl 12.41\n",
            "| epoch   5 step    24800 |   3768 batches | lr 1.16e-05 | ms/batch 583.14 | loss  2.83 | avg loss  2.47 | ppl 11.87\n",
            "| epoch   5 step    24900 |   3868 batches | lr 1.08e-05 | ms/batch 583.19 | loss  2.32 | avg loss  2.48 | ppl 11.92\n",
            "| epoch   5 step    25000 |   3968 batches | lr 1e-05 | ms/batch 583.33 | loss  2.36 | avg loss  2.48 | ppl 11.94\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.25000.pt\n",
            "| epoch   5 step    25100 |   4068 batches | lr 9.23e-06 | ms/batch 583.53 | loss  2.59 | avg loss  2.47 | ppl 11.77\n",
            "| epoch   5 step    25200 |   4168 batches | lr 8.45e-06 | ms/batch 583.36 | loss  2.35 | avg loss  2.49 | ppl 12.07\n",
            "| epoch   5 step    25300 |   4268 batches | lr 7.68e-06 | ms/batch 583.45 | loss  2.45 | avg loss  2.46 | ppl 11.76\n",
            "| epoch   5 step    25400 |   4368 batches | lr 6.9e-06 | ms/batch 583.49 | loss  2.68 | avg loss  2.45 | ppl 11.58\n",
            "| epoch   5 step    25500 |   4468 batches | lr 6.13e-06 | ms/batch 583.36 | loss  2.51 | avg loss  2.48 | ppl 11.89\n",
            "| epoch   5 step    25600 |   4568 batches | lr 5.35e-06 | ms/batch 583.50 | loss  2.69 | avg loss  2.48 | ppl 11.99\n",
            "| epoch   5 step    25700 |   4668 batches | lr 4.58e-06 | ms/batch 583.67 | loss  2.77 | avg loss  2.53 | ppl 12.50\n",
            "| epoch   5 step    25800 |   4768 batches | lr 3.8e-06 | ms/batch 583.30 | loss  2.53 | avg loss  2.50 | ppl 12.19\n",
            "| epoch   5 step    25900 |   4868 batches | lr 3.02e-06 | ms/batch 583.21 | loss  2.35 | avg loss  2.47 | ppl 11.84\n",
            "| epoch   5 step    26000 |   4968 batches | lr 2.25e-06 | ms/batch 583.59 | loss  2.43 | avg loss  2.49 | ppl 12.06\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.26000.pt\n",
            "eval samples: 0 loss: tensor(1.1779, device='cuda:0')\n",
            "eval samples: 100 loss: tensor(0.8779, device='cuda:0')\n",
            "eval samples: 200 loss: tensor(1.2444, device='cuda:0')\n",
            "eval samples: 300 loss: tensor(1.0935, device='cuda:0')\n",
            "eval samples: 400 loss: tensor(0.8851, device='cuda:0')\n",
            "eval samples: 500 loss: tensor(0.5419, device='cuda:0')\n",
            "eval samples: 600 loss: tensor(1.1545, device='cuda:0')\n",
            "eval samples: 700 loss: tensor(0.6526, device='cuda:0')\n",
            "eval samples: 800 loss: tensor(1.1132, device='cuda:0')\n",
            "eval samples: 900 loss: tensor(1.7305, device='cuda:0')\n",
            "eval samples: 1000 loss: tensor(0.9206, device='cuda:0')\n",
            "eval samples: 1100 loss: tensor(1.4974, device='cuda:0')\n",
            "average loss 1.1675086807561657\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Eval  13 at step    26000 | time: 159.00s | valid loss  1.17 | valid ppl  3.21 | best ppl  3.21 \n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   5 step    26100 |   5068 batches | lr 1.47e-06 | ms/batch 2173.62 | loss  2.48 | avg loss  2.47 | ppl 11.87\n",
            "| epoch   5 step    26200 |   5168 batches | lr 6.98e-07 | ms/batch 583.45 | loss  2.64 | avg loss  2.47 | ppl 11.76\n",
            "saving checkpoint ./trained_models/GPT2_M/e2e/model.26290.pt\n",
            "----------------------------------------------------------------------------------------------------\n",
            "End of training\n",
            "cleanup dist ...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}